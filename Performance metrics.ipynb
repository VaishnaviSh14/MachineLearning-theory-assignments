{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6538a7b6-64ae-4d72-ae06-d397028a48d0",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "Ans.\n",
    "Grid search CV is a technique for hyperparameter tuning. It works by searching through a grid of hyperparameter values and evaluating the model performance on each combination of hyperparameters. The hyperparameters that produce the best model performance are then used to train the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc33a5-c340-43d4-b688-0220f3b66c78",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?\n",
    "\n",
    "Ans.\n",
    "Grid search CV and random search CV are both techniques for hyperparameter tuning. The main difference between the two is that grid search CV tests all possible combinations of hyperparameter values, while random search CV tests a random subset of the possible combinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d7944-4d25-4096-9ccc-e390b9420372",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "Ans.\n",
    "\n",
    "Data leakage is a problem that occurs when the training data is used to make predictions on the test data. This can happen when the training data and test data are not independent of each other.\n",
    "\n",
    "For example, let's say you are building a machine learning model to predict the price of houses. You might train your model on data that includes the current price of the house. However, if you also include the future price of the house in the training data, then your model will be able to predict the future price of the house by simply looking at the current price. This is an example of data leakage.\n",
    "\n",
    "Data leakage can lead to overfitting, which is when the model learns the training data too well and does not generalize well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22439d-4449-4bb8-b8fe-733f12cd52c1",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "Ans.\n",
    "\n",
    "There are a few ways to prevent data leakage when building a machine learning model. One way is to split the data into two sets: a training set and a test set. The training set is used to train the model, and the test set is used to evaluate the model performance. The training set and test set should be independent of each other, so that the model does not learn the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f9b0f-9128-4564-9bd5-9eedab645caf",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "\n",
    "Ans.\n",
    "A confusion matrix is a table that summarizes the performance of a classification model. It shows the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model.\n",
    "The confusion matrix can be used to calculate a number of metrics to evaluate the performance of the model, such as accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f26b66-31c3-457e-9373-ba2f89d115dc",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "Ans.\n",
    "Precison - Out of all predicted values how many are correctly predicted\n",
    "We use this when FP in important\n",
    "\n",
    "recall - Out of all predicted values how many are correctly predicted with actual values.\n",
    "we use this when FN is important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0b929a-cebb-43a6-bff4-c8d5f2933e9b",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "Ans.\n",
    "To interpret a confusion matrix to determine which types of errors your model is making, you can look at the following:\n",
    "\n",
    "The number of true positives (TP): This is the number of instances that were correctly classified as positive.\n",
    "The number of false positives (FP): This is the number of instances that were incorrectly classified as positive.\n",
    "The number of true negatives (TN): This is the number of instances that were correctly classified as negative.\n",
    "The number of false negatives (FN): This is the number of instances that were incorrectly classified as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020e1fd-6dd8-4285-a0bb-f4edf0fb5c49",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Accuracy: Accuracy is the overall measure of how well the model performs. It is calculated by dividing the number of correct predictions by the total number of predictions.\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision: Precision is the fraction of positive predictions that are actually positive. It is calculated by dividing the number of true positives by the number of true positives plus the number of false positives.\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall: Recall is the fraction of actual positives that are correctly predicted as positive. It is calculated by dividing the number of true positives by the number of true positives plus the number of false negatives.\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 score: The F1 score is a measure of both precision and recall. It is calculated by taking the harmonic mean of precision and recall.\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d686e1-5012-46f7-b2f7-e77df595c678",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "Ans.\n",
    "The accuracy of a model is the overall measure of how well the model performs. It is calculated by dividing the number of correct predictions by the total number of predictions.\n",
    "\n",
    "The values in the confusion matrix can be used to calculate the accuracy of the model. The accuracy is equal to the sum of the true positives (TP) and true negatives (TN) divided by the total number of predictions.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac244da-47ab-421a-90cd-418f279a6b7c",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?\n",
    "\n",
    "Ans.\n",
    "A confusion matrix can be used to identify potential biases or limitations in your machine learning model by looking at the following:\n",
    "\n",
    "The number of false positives (FP): This is the number of instances that were incorrectly classified as positive. A high number of FP can indicate that the model is biased towards predicting the positive class.\n",
    "\n",
    "The number of false negatives (FN): This is the number of instances that were incorrectly classified as negative. A high number of FN can indicate that the model is biased towards predicting the negative class.\n",
    "\n",
    "The difference between the precision and recall: A large difference between the precision and recall can indicate that the model is biased towards one class or the other.\n",
    "\n",
    "The distribution of the errors: If the errors are not evenly distributed across the classes, then this can indicate that the model is biased towards one class or the other.\n",
    "\n",
    "For example, let's say you are building a model to classify images of cats and dogs. If the model has a high number of FP, then this could indicate that the model is biased towards predicting that an image is a cat, even if it is actually a dog. This could be due to the fact that there are more images of cats in the training data than images of dogs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
