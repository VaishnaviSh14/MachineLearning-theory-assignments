{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d794f9-5fa3-4917-88cb-3466fb314e42",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "Ans.\n",
    "R-squared is a goodness-of-fit measure for linear regression models. \n",
    "In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
    "\n",
    "R^2 = 1 -( SS RES / SS Total)\n",
    "\n",
    "R^2 = 1 − ∑ ( y i − y i ^ ) 2 /∑ ( y i − y¯i ) 2 \n",
    "\n",
    "SS RES = sum of squared residuals\n",
    "SS total = sum of squared total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bcd96-8080-4934-8d6f-d27ad6341bbf",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Ans.\n",
    "\n",
    "Adjusted R-squared is a modification of the regular R-squared that takes into account the number of independent variables used in the model.\n",
    "R-squared, on the other hand, does have its limitations. One of the most essential limits to using this model is that R-squared cannot be used to determine whether or not the coefficient estimates and predictions are biased. Furthermore,  in multiple linear regression, the R-squared cannot tell us which regression variable is more important than the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ee06e-0f3e-41c6-b314-3e4f0011d160",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Ans.\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of independent variables. It helps to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af784785-11ac-4c2c-8576-edb3834d8ab6",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "Ans.\n",
    "These are common metrics used to evaluate the performance of regression models.\n",
    "Root Mean Squared Error (RMSE): It measures the average of the squared differences between predicted and actual values, and then takes the square root. It emphasizes larger errors due to squaring.\n",
    "Mean Squared Error (MSE): It's similar to RMSE but without taking the square root. It's the average of the squared differences.\n",
    "Mean Absolute Error (MAE): It measures the average of the absolute differences between predicted and actual values. It treats all errors equally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a87f3-b19b-48c0-b438-198afac50b7d",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "Ans.\n",
    "\n",
    "MSE - \n",
    "Advantages -\n",
    "1. It is differential ,at any point we can calculate slope or derivative.\n",
    "2. It has one local and one global minima.\n",
    "\n",
    "Disadvantages - \n",
    "1. Not robust to outliers.\n",
    "2. It is not in same unit.\n",
    "\n",
    "\n",
    "MAE - \n",
    "Advantages -\n",
    "1. Robust to outliers.\n",
    "2. It is in same unit.\n",
    "\n",
    "Disadvantages - \n",
    "1. Convergence takes time , optimisation is complex.\n",
    "2. Time consuming.\n",
    "\n",
    "RMSE - \n",
    "Advantages -\n",
    "1. It is in same unit\n",
    "2. Differentiable\n",
    "\n",
    "Disadvantages - \n",
    "1. Not robust to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b47fe8-8990-43c9-b7ae-39e62c6cb784",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Ans.\n",
    "Lasso regression (Least Absolute Shrinkage and Selection Operator) is a regularization technique that not only helps in reducing overfitting but also has the unique property of performing feature selection.\n",
    "\n",
    "It is particularly useful when you suspect that many of the features in your dataset are irrelevant or redundant.\n",
    "Lasso regression and Ridge regression are both regularization techniques used in linear regression to address overfitting, but they differ in the way they apply the regularization and the effects they have on the model's coefficients.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590608f4-3ae0-4550-bd1f-810b56e965cc",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "Ans.\n",
    "\n",
    "Regularized linear models like Ridge and Lasso help prevent overfitting by controlling the magnitude of coefficients. When the regularization term is added to the cost function, it discourages the model from fitting the training data too closely, which in turn reduces the likelihood of capturing noise in the data.\n",
    "\n",
    "Example: Imagine a dataset where there are many features, some of which are noisy and contribute little to the target variable. Without regularization, the model might assign non-zero coefficients to these features, causing overfitting. Regularization can help shrink these coefficients toward zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9433cb-7f8c-48c2-96d4-6dc60a9429fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Ans.\n",
    "They include all the predictors in the final model.\n",
    "They shrink the coefficients towards zero.\n",
    "They trade the variance for bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c79e3-54e9-47b5-b2c0-86f2a2cfcc12",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "Ans.\n",
    "\n",
    "I would choose Model B as the better performer.\n",
    "In this case, Model A has an RMSE of 10, while Model B has an MAE of 8. This means that Model B's predictions are, on average, closer to the actual values than Model A's predictions. This is because MAE is less sensitive to outliers, and there are no outliers in the given data.\n",
    "\n",
    "However, there are some limitations to using MAE as an evaluation metric. MAE is not as sensitive to the scale of the data as RMSE. This means that if the data is in different units, MAE may not be a fair comparison between the two models. Additionally, MAE does not take into account the distribution of the errors. This means that two models with the same MAE could have very different distributions of errors, which could lead to different results in practice.\n",
    "\n",
    "Here are some additional considerations when choosing an evaluation metric for regression models:\n",
    "\n",
    "The scale of the data: If the data is in different units, RMSE may be a better choice than MAE.\n",
    "The distribution of the errors: If the errors are not normally distributed, MAE may not be a good choice.\n",
    "The purpose of the model: If the model is being used to make decisions with high stakes, RMSE may be a better choice than MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9e286-ea58-40df-8fb2-2fee1f36ee5f",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "Ans.\n",
    "In this case, Model A uses ridge regularization with a regularization parameter of 0.1. This means that the model will be less sensitive to the individual values of the coefficients, and the overall variance of the model will be reduced. Model B uses lasso regularization with a regularization parameter of 0.5. This means that the model will be more likely to set some of the coefficients to zero, which can help with feature selection.\n",
    "\n",
    "If the goal is to reduce the variance of the model, then Model A would be the better choice. However, if the goal is to perform feature selection, then Model B would be the better choice.\n",
    "\n",
    "There are some trade-offs and limitations to both regularization methods. Ridge regularization can make the model less interpretable, since it can shrink the coefficients of important features. Lasso regularization can also make the model less interpretable, since it can set some of the coefficients to zero. Additionally, both regularization methods can make the model less flexible, which can lead to underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
